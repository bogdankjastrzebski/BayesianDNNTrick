{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820b983c-7bbc-4bef-8cf5-14e20d9f34f8",
   "metadata": {},
   "source": [
    "# Invertible Neural Networks (for generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "027f0143-3f28-4218-8800-e59631120f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as np\n",
    "import jax.scipy as sp\n",
    "import numpy as onp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fcd25c-3cdd-4e00-889e-4bf4b0fe9d60",
   "metadata": {},
   "source": [
    "$$\\frac{|| f(x_0) - f(x_1) ||}{||x_0 - x_1||} < q \\qquad \\forall_{x_0, x_1 \\in X}$$\n",
    "$$ z = f(x) + x $$\n",
    "$$ x = z - f(x) $$\n",
    "$$ x = z - f(z - f(x))$$\n",
    "$$ x = z - f(z - f(\\dots))$$\n",
    "\n",
    "Is $g(x) = z - f(x)$ a contraction? \n",
    "\n",
    "$$|| g(x_0) - g(x_1) || = || z - f(x_0) - z + f(x_1) || $$\n",
    "$$ = || f(x_1) - f(x_0) || = $$\n",
    "$$ = || f(x_1) - f(x_0) || < q ||x_0 - x_1|| $$\n",
    "\n",
    "Therefore, $g$ is a contraction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b9359-412e-4a74-86ac-72cf5631f3ba",
   "metadata": {},
   "source": [
    "Let $X \\sim \\mathcal{X}$. \n",
    "$$ p(f | D) = \\frac{p(D | f)p(f)}{p(D)} \\propto p(D | f)p(f) $$\n",
    "$$ \\mathbb{D}_{KL}[q(f)\\;||\\;p(f | D)] = -\\mathbb{E}_{f \\sim q, X \\sim \\mathcal{X}}\\text{log}(f(X)) + \\mathbb{D}_{KL}[q(f) \\;||\\;p(f)]$$\n",
    "Easier, not accurate:\n",
    "$$ \\max_{\\theta \\in \\Theta} \\mathbb{E}_{X\\sim\\mathcal{X}}\\text{log}\\,m_\\theta(X) $$\n",
    "$$ \\text{s.t. } \\forall_{w \\in W} ||w||_2 < q $$\n",
    "where $0 \\leq q < 1$ and $W \\subset \\theta$ are the weights of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292a54d-be26-4e24-a8ff-1ce5e5aeb2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9cffeb2a-4dca-4797-b671-34a52b0da448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layer(key, input_dim, output_dim, activation_function=jax.nn.relu):\n",
    "    weight = jax.random.normal(key, shape=(input_dim, output_dim))\n",
    "    bias = np.zeros((output_dim,))\n",
    "    return weight, bias, activation_function\n",
    "\n",
    "def create_mlp(key, inout):\n",
    "    return [create_layer(key, inout[i], inout[i+1])\n",
    "            for i, key in enumerate(jax.random.split(key, len(inout)-1))]\n",
    "\n",
    "def layer_forward(layer, X):\n",
    "    weight, bias, activation_function = layer\n",
    "    return activation_function(X @ weight + bias)\n",
    "\n",
    "def residual_forward(layer, X):\n",
    "    return layer_forward(layer, X) + X\n",
    "\n",
    "def forward(model, X):\n",
    "    for layer in model:\n",
    "        X = residual_forward(layer, X)\n",
    "    return X\n",
    "\n",
    "def layer_constraint(layer):\n",
    "    weight, _, _ = layer\n",
    "    return np.linalg.norm(weight, ord=2)\n",
    "\n",
    "def constraint(model):\n",
    "    return sum(layer_constraint(layer) for layer in model)\n",
    "\n",
    "def loss(model):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a049c-0836-4b71-a9ae-07e143a1e72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
