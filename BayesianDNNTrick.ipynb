{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7fac9cc-8aa4-4c35-942c-a0c89ef2d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "import jax.scipy as sp\n",
    "import jax\n",
    "from jax import jit, grad, vmap\n",
    "\n",
    "# from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9991ac-fab4-4673-b117-4032075b7cd8",
   "metadata": {},
   "source": [
    "### The Idea\n",
    "\n",
    "In Bayesian statistics, we search for a posterior distribution of parameters:\n",
    "$$p(\\theta | D) = \\frac{p(D | \\theta)p(\\theta)}{p(D)} \\propto p(D | \\theta)p(\\theta)$$\n",
    "\n",
    "But, if the parameters represent a function, i.e., they are parameters of the function, maybe what we are looking for is the posterior probability over **functions**:\n",
    "\n",
    "$$p(f | D) = \\frac{p(D | f)p(f)}{p(D)} \\propto p(D | f)p(f)$$\n",
    "\n",
    "$$p(D | f) = p(X, Y | f) = p(Y | X, f)p(X | f) = p(Y | X, f)p(X)$$\n",
    "\n",
    "$$p(D) = p(X, Y) = p(Y | X)p(X) $$\n",
    "\n",
    "$$p(f | D) = \\frac{p(D | f)p(f)}{p(D)} = \\frac{p(Y | X, f)p(X)p(f)}{p(Y | X)p(X)} = \\frac{p(Y | X, f)p(f)}{p(Y | X)}$$\n",
    "\n",
    "$$\\mathbb{D}_{KL}[q(f | D) | p(f | D)] = -\\mathbb{E}_{f \\sim q}\\text{log}\\frac{p(f|D)}{q(f|D)} = -\\mathbb{E}_{f \\sim q}\\text{log}\\frac{p(Y | X, f)p(X | f)p(f)}{q(f|D)} = -\\mathbb{E}_{f \\sim q}\\text{log}p(Y | X, f) - \\mathbb{E}_{f \\sim q}\\text{log}\\frac{p(f)}{q(f|D)} + C$$\n",
    "\n",
    "$$\\mathbb{D}_{KL}[q(f | D) | p(f | D)] = -\\mathbb{E}_{f \\sim q}\\text{log}p(Y | X, f) + \\mathbb{D}_{KL}[q(f|D) | p(f)] + C $$\n",
    "\n",
    "$$\\mathbb{D}_{KL}[q(f | D) | p(f | D)] = -\\mathbb{E}_{f \\sim q}\\text{log}p(Y | X, f) + \\mathbb{H}[q(f|D), p(f)] - \\mathbb{H}[q(f|D)] + C $$\n",
    "\n",
    "The relative enropy $\\mathbb{H}[q(f|D), p(f)] = -\\mathbb{E}_{f \\sim q}\\text{log}p(f)$ is simple to calculate, since we can sample from $q$ and we choose our prior. Having a sampler, we can also easily evaluate the first term, the cross-entropy \"loss\". However, to calculate the second term, the entropy of $q$, we need the surrogate posterior $q$. The trick is to use the derivative of our sampler instead:\n",
    "\n",
    "$$\\text{log}\\frac{d}{dp}F^{-1}(p) = \\text{log}\\frac{1}{F'(F^{-1}(p))} = \\text{log}\\frac{1}{q(f|D)} = -\\text{log}q(f|D)$$\n",
    "since inverse of cdf $F^{-1}$ is a sampler (!!!) then $F^{-1}=f$ and $F'$ is density function, so $F'=q(\\cdot|D)$. Therefore, we can calculate the entropy of $q$ as follows:\n",
    "\n",
    "$$H(q) = -\\mathbb{E}_{f \\sim q}\\text{log}q(f | D)= -\\mathbb{E}_{p \\sim U}\\text{log}q(F^{-1}(p) | D) = \\mathbb{E}_{f \\sim q}\\text{log}\\frac{d}{dp}F^{-1}(p)$$\n",
    "\n",
    "where the $F^{-1}: p \\rightarrow (\\mathcal{X} \\rightarrow \\mathcal{Y}) $ returns a neural network, $\\mathcal{X}$ is input space, $\\mathcal{Y}$ is the output space, $F^{-1}(p) = f$ and $p \\sim U$ is the source of randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e6fd6-3f7f-4ad7-8c9c-50d7e15c8094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
